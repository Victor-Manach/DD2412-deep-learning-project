- Semantically it is comparable to vmap() because both transformations map a function over array axes
- The mapped axis size must be less than or equal to the number of local XLA devices available, as returned by jax.local_device_count()
  In anticipation of spreading the batch across several devices, we’ll make the batch size equal to the number of devices

- pmap() compiles functions, so while it can be combined with jit(), it’s usually unnecessary.
  pmap(value_and_grad(fun[, argnums, has_aux, ...]))

#FROM https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html
@functools.partial(jax.pmap, axis_name='num_devices')
def update(params: Params, xs: jnp.ndarray, ys: jnp.ndarray) -> Tuple[Params, jnp.ndarray]:
  """Performs one SGD update step on params using the given data."""

  # Compute the gradients on the given minibatch (individually on each device).
  loss, grads = jax.value_and_grad(loss_fn)(params, xs, ys)

  # Combine the gradient across all devices (by taking their mean).
  grads = jax.lax.pmean(grads, axis_name='num_devices')

  # Also combine the loss. Unnecessary for the update, but useful for logging.
  loss = jax.lax.pmean(loss, axis_name='num_devices')

  # Each device performs its own update, but since we start with the same params
  # and synchronise gradients, the params stay in sync.
  new_params = jax.tree_map(
      lambda param, g: param - g * LEARNING_RATE, params, grads)

  return new_params, loss